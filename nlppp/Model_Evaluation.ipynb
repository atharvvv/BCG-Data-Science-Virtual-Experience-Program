{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c90b2b5b",
      "metadata": {
        "id": "c90b2b5b"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca75315",
      "metadata": {
        "id": "2ca75315"
      },
      "source": [
        "#### 1. Translation:\n",
        "Metrics:\n",
        "- BLEU Score: Measures the overlap in n-grams between the generated translation and reference translations.\n",
        "\n",
        "Standard Range: 0 to 1\n",
        "\n",
        "Interpretation: Higher values indicate better overlap with reference translations.\n",
        "\n",
        "\n",
        "- ROUGE Score: Evaluates the overlap in n-grams, word sequences, and word overlap between generated and reference translations.\n",
        "\n",
        "Standard Range: 0 to 1\n",
        "\n",
        "Interpretation: Higher values suggest better overlap in n-grams, word sequences, and word overlap.\n",
        "- METEOR Score: Considers precision, recall, stemming, synonymy, stemming, and word order.\n",
        "\n",
        "Standard Range: 0 to 1\n",
        "\n",
        "Interpretation: Higher values indicate better overall performance considering precision, recall, stemming, synonymy, and word order.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Summarization:\n",
        "Metrics:\n",
        "- ROUGE Score: Measures the overlap in n-grams, word sequences, and word overlap between generated and reference summaries.\n",
        "\n",
        "Standard Range: 0 to 1\n",
        "\n",
        "Interpretation: Higher values suggest better overlap in n-grams, word sequences, and word overlap.\n",
        "- BLEU Score: Can also be used for evaluating summarization tasks.\n",
        "\n",
        "Standard Range: 0 to 1\n",
        "\n",
        "Interpretation: Higher values indicate better overlap with reference summaries.\n",
        "\n",
        "\n",
        "#### 3. Text Generation:\n",
        "Metrics:\n",
        "- Perplexity: Measures the uncertainty of a language model on a given text.\n",
        "\n",
        "Standard Range: Lower is better; no strict upper bound.\n",
        "\n",
        "Interpretation: Lower values indicate better language model performance on the given text.\n",
        "\n",
        "- Diversity Metrics: Evaluate the diversity of generated text (e.g., uniqueness of generated responses).\n",
        "\n",
        "Standard Range: Context-dependent; aim for diversity.\n",
        "\n",
        "Interpretation: Higher diversity indicates more unique and varied generated responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07560b3",
      "metadata": {
        "id": "a07560b3"
      },
      "source": [
        "### Translation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574a50ad",
      "metadata": {
        "id": "574a50ad",
        "outputId": "527bb175-428d-43ca-fa8f-8a0f6b8f6eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "METEOR Score: 0.6148\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Sample Input\n",
        "reference_sentences = [\"The cat is on the mat.\"]\n",
        "hypothesis_sentence = \"A cat is lying on the carpet.\"  #generated by a model\n",
        "\n",
        "# Convert to METEOR's expected format\n",
        "reference_sentences = [reference_sentence.split() for reference_sentence in reference_sentences]\n",
        "hypothesis_sentence = hypothesis_sentence.split()\n",
        "\n",
        "# METEOR Score Calculation\n",
        "score = meteor_score(reference_sentences, hypothesis_sentence)\n",
        "print(f\"METEOR Score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61186e3",
      "metadata": {
        "id": "e61186e3"
      },
      "source": [
        "A METEOR score of 0.6148 indicates a moderate level of quality in the translation output. It suggests that the translation system has achieved a reasonable alignment with reference translations, considering various linguistic factors such as precision, recall, stemming, synonymy, and word order."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e60f5607",
      "metadata": {
        "id": "e60f5607"
      },
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4d6774",
      "metadata": {
        "id": "bb4d6774",
        "outputId": "8e627d40-4068-4cb5-8a1d-de53591ec7e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in c:\\users\\vandana\\anaconda3\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: six in c:\\users\\vandana\\anaconda3\\lib\\site-packages (from rouge) (1.16.0)\n",
            "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)'))) - skipping\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059087fd",
      "metadata": {
        "id": "059087fd",
        "outputId": "61b769ca-b7ce-4b81-b101-80f5b72ccb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 Score: 0.8696\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Generated Summary\n",
        "generated_summary = \"Scientists excited about discovery of new planet in distant galaxy.\"\n",
        "\n",
        "# Reference Summaries\n",
        "reference_summaries = [\"Scientists are excited about the discovery of a new planet in a distant galaxy.\"]\n",
        "\n",
        "# ROUGE Score\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(generated_summary, reference_summaries[0])\n",
        "print(f\"ROUGE-1 Score: {rouge_scores[0]['rouge-1']['f']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a329bd92",
      "metadata": {
        "id": "a329bd92"
      },
      "source": [
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "\n",
        "A ROUGE-1 Score of 0.8696 indicates a high level of overlap in unigrams between the generated summary and the reference summaries. The higher the ROUGE-1 Score, the better the generated summary aligns with the reference summaries in terms of single-word sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b19dedcb",
      "metadata": {
        "id": "b19dedcb"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "Perplexity is a measure of how well the model predicts the input sequence, and lower values are indicative of better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec38e8d2",
      "metadata": {
        "id": "ec38e8d2",
        "outputId": "cb4d98f4-ed78-4211-a993-c0abec32c792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 20329.5020\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize GPT-2 model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Generated Text\n",
        "generated_text = \"In a world where robots have emotions, they express joy and sorrow just like humans.\"\n",
        "\n",
        "# Tokenize and calculate Perplexity\n",
        "input_ids = tokenizer.encode(generated_text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids).logits\n",
        "\n",
        "# Calculate Perplexity\n",
        "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "perplexity = torch.exp(torch.nn.functional.cross_entropy(logits.squeeze(0), input_ids.squeeze(0)))\n",
        "\n",
        "print(f\"Perplexity: {perplexity.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c964f6c8",
      "metadata": {
        "id": "c964f6c8"
      },
      "source": [
        "Perplexity is a measure of how well a probability distribution or probability model predicts a sample. In the context of language models like GPT-2, perplexity is commonly used to evaluate how well the model predicts a sequence of tokens (words or subwords).\n",
        "\n",
        "\n",
        "Here's what your obtained perplexity value of 20329.5020 means:\n",
        "\n",
        "- The model's estimated probability of the sequence of tokens is equivalent to the probability of a random event with a perplexity of 20329.5020.\n",
        "\n",
        "- Higher perplexity values indicate higher uncertainty or poorer performance. In an ideal case, the perplexity would be close to the actual number of possible outcomes (vocabulary size), resulting in a perplexity around 1.\n",
        "\n",
        "- In natural language processing, perplexity is often used as an evaluation metric for language models. A lower perplexity suggests that the model assigns higher probabilities to the observed sequence of tokens.\n",
        "\n",
        "Overall, a perplexity of 20329.5020 means that, on average, the model is assigning a relatively low probability to the correct sequence of tokens, indicating room for improvement in terms of language modeling performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275119c2",
      "metadata": {
        "id": "275119c2",
        "outputId": "5302ad6c-5be8-454b-d565-1ac3ce373ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: I enjoy learning new things.\n",
            "Perplexity: 7027.2896\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize GPT-2 model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Generated Text\n",
        "generated_text = \"I enjoy learning new things.\"\n",
        "\n",
        "# Tokenize and calculate Perplexity\n",
        "input_ids = tokenizer.encode(generated_text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids).logits\n",
        "\n",
        "# Calculate Perplexity\n",
        "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "perplexity = torch.exp(torch.nn.functional.cross_entropy(logits.squeeze(0), input_ids.squeeze(0)))\n",
        "\n",
        "print(f\"Generated Text: {generated_text}\")\n",
        "print(f\"Perplexity: {perplexity.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a188ec",
      "metadata": {
        "id": "b5a188ec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}